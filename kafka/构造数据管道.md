### 构建数据管道

#### 构建数据管道时需要考虑的问题

1. 及时性

有些系统希望每天一次性地接收大量的数据，而有些希望在数据生成几毫秒内就能拿到它们，一个好的数据集成系统能够很好的支持数据管道的各种及时性需求，而且在业务需求发生变更时，具有不同及时性需求的数据表之间可以方便的迁移。

2. 可靠性

我们要避免单点故障，并且能够自动从各种故障中快速恢复。

3. 高吞吐量和动态吞吐量

数据管道需要支持非常高的吞吐量，在某些情况下还需要应对突发的吞吐量增长。在kafka中，如果生成者的吞吐量超过了消费者的吞吐量，那么可以把数据积压在Kafka中，等待消费者追赶上来。通过增加额外的消费者或者生成者可以实现Kafka的伸缩。

4. 数据格式

数据管道需要协调各种数据格式和数据类型，数据类型取决于不同的数据库和数据存储系统。

5. 转换

数据管道的构建可以分为两大阵营：ETL和ELT。

ETL表示**提取－转换－加载**(Extract-Transform-Load)，也就是说，当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据的过程。不过它也有一个不好的地方，如：假如有人在MongoDB和MySql之间建立了数据管道，并且移除了一些字段，那么下游应用从MySql中访问到的数据是不完整的。如果想要访问那些被移除的字段，只能重新构建管道。

ELT表示**提取-加载－转换**(Extract-Load-Transform)，在这种模式下，数据管道只做少量的转换(主要是数据类型转换)，确保到达数据池的数据尽可能的与数据源保持一致。目标系统收集“原始数据”，并负责处理它们。

6. 安全性

对于安全性我们主要考虑下面几个问题：

* 能否将流经数据管道的数据加密
* 谁能修改数据管道
* 如果数据管道需要从一个不受信任的位置读取或者写入数据，是否有适当的认证机制

Kafka支持加密传输数据，从数据源到Kafka，再从Kafka到数据池。它还支持认证（通过SASL来实现）和授权。Kafka还提供了审计日志用于追踪访问日志，通过编写额外的代码，还能追踪到每个事件的来源和事件的修改者。

7. 耦合性和灵活性


​
